summary for today: analyzing algorithms to find big-o notation and time complexity. Using math to determine big-o by using the theoretical real-RAM computing model

finding max and second_max in an unsorted array
	(continuing from [[USU/CS5050 - Advanced Algorithms/2025.01.07|2025.01.07]])
	n = 2^k where k is the depth of the imaginary tree structure
	second_max will have lost to the max at some point
	backtrack all competitions of max vs others
	keep track of all competitions of max vs others
	need to skeep track of k comparisons which is logn
	for max we need n comparisons 
	for second_max we need logn comparisons
	in total we have n + logn comparisons to find max and second_max
	we need some system to come up with big O notation
what is a good algorithm?
	1) correct
	2) use less resources (time, space, others)
	how do we know an algorithm is good?
		#term algorithm analysis
	example: sorting algorithm
		input: array of n elements
		output: sorted array in ascending order
		#term insertion sort
			when you iterate through an unsorted array and 'insert' elements into where they should be in comparison to other elements to sort it
			pseudocode
				for i=2 to n
					b = A\[i]  \# current element of interest
					k = i - 1
					while b < A\[k] and k >= 1
						A\[k + 1] = A\[k]
						k = k - 1
					A\[k + 1] = b
			worst case is algorithm has to first of all iterate through every element, and for each element iterate back through the entire sorted array to find the correct insertion spot = big O of 2(N)
solution for finding big O
	issue: dependance on the input size
	solution: define a time function T(n) of the input size n
	issue: dependance on the machine
	solution: use the hypothetical #term real-RAM as the computing model
		each real number is stored in a unit space
		given the address of the data, access is in constant time
		primitive operations take constant time (division, addition, etc)
	solution summary: use a function of input size n to describe the running time of the algorithm of the worst case under the theoretical real-RAM machine
		in previous example this would look like saying if C is how we represent constant time, then on the last line there would be 3 C's because accessing A is an operation, adding k + 1 is an operation, and assigning it to b is another operation
		so in total there were 20 C's, multiplied by size of array (as per for loop) so big O of insertion sort was 
growth of function
	1) drop the low order items (big O element with largest exponent is the only one we really care about)
	2)  ignore the leading constant (the coefficient of the leading item also doesn't really matter)
	example: 2n^2 + 4n ---> n^2
exercises for algorthm analysis
	example1 
		honestly tried writing down math in notebook and got lost
	example 2
		basically 2 nested for loops means n^2 
	example 3 
		if n isnt used, time complexity is o(1)
		even if the amount of elements is very large, it is still always better than having something like logn or n^2 for an equal number of elements n
		since n can grow even logn is always worse than big o of 1
	example 4
Asymptotic notation: big-O
	two positive functions: f(n) and g(n)
	big-o: f(n) = O(g(n)) if there ixists a number n0 and a constant c such that f(n) <= c ... too slow i guess 
	big O provides an asymptotic upper bound for f(n)
	yeah no idea what the hell bro is talking about its some math stuff
		
		
		

